{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow -q\n",
    "%pip install gymnasium -q\n",
    "%pip install keras -q\n",
    "%pip install keras-rl2 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create the Environment\n",
    "\n",
    "This is the OpenAI Gym `Cartpole-v1` environment. The goal is to balance a pole upright and stay within a certain horizontal range for as long as possible. A reward of 1 point is given for each step taken where the pole does not fall or the cart does not go out of bounds. Read more about the environment [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: (4,)\n",
      "Action Space: 2\n"
     ]
    }
   ],
   "source": [
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f\"Observation Space: {input_shape}\")\n",
    "print(f\"Action Space: {num_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state:  (array([-0.00571921,  0.03531359,  0.02441932, -0.04033282], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial state: \", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools.dist\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model(input_shape, num_actions):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(shape=input_shape),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(24, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_actions, activation='linear')  # Outputs Q-values for each action, hence two linear nodes\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(input_shape, num_actions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "learning_rate = 0.0002\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_min = 0.1  # Final exploration rate\n",
    "epsilon_decay = 0.999\n",
    "batch_size = 32\n",
    "memory_size = 5000\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "replay_memory = deque(maxlen=memory_size)\n",
    "target_model = create_model(input_shape, num_actions)\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "#Target model is not compiled because it's weights are not getting updated during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Epsilon-greedy policy for action selection -> Read more here: https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/\n",
    "def choose_action(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)  # Explore\n",
    "    q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "    return np.argmax(q_values[0])  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Train the model using a batch of experiences\n",
    "def train_model():\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return  # Wait until enough experiences are stored\n",
    "\n",
    "    batch = random.sample(replay_memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "\n",
    "    # Compute target Q-values\n",
    "    next_q_values = target_model.predict(next_states, verbose=0)\n",
    "    max_next_q_values = np.max(next_q_values, axis=1)\n",
    "    target_q_values = rewards + gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "    # Update the model\n",
    "    q_values = model.predict(states, verbose=0)\n",
    "    for i, action in enumerate(actions):\n",
    "        q_values[i, action] = target_q_values[i]\n",
    "\n",
    "    model.fit(states, q_values, epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 10.0\n",
      "Episode 2: Total Reward = 42.0\n",
      "Episode 3: Total Reward = 13.0\n",
      "Episode 4: Total Reward = 26.0\n",
      "Episode 5: Total Reward = 13.0\n",
      "Episode 6: Total Reward = 8.0\n",
      "Episode 7: Total Reward = 18.0\n",
      "Episode 8: Total Reward = 15.0\n",
      "Episode 9: Total Reward = 38.0\n",
      "Episode 10: Total Reward = 15.0\n",
      "Episode 11: Total Reward = 30.0\n",
      "Episode 12: Total Reward = 53.0\n",
      "Episode 13: Total Reward = 38.0\n",
      "Episode 14: Total Reward = 14.0\n",
      "Episode 15: Total Reward = 19.0\n",
      "Episode 16: Total Reward = 14.0\n",
      "Episode 17: Total Reward = 23.0\n",
      "Episode 18: Total Reward = 17.0\n",
      "Episode 19: Total Reward = 11.0\n",
      "Episode 20: Total Reward = 36.0\n",
      "Episode 21: Total Reward = 11.0\n",
      "Episode 22: Total Reward = 36.0\n",
      "Episode 23: Total Reward = 28.0\n",
      "Episode 24: Total Reward = 13.0\n",
      "Episode 25: Total Reward = 17.0\n",
      "Episode 26: Total Reward = 28.0\n",
      "Episode 27: Total Reward = 42.0\n",
      "Episode 28: Total Reward = 21.0\n",
      "Episode 29: Total Reward = 11.0\n",
      "Episode 30: Total Reward = 12.0\n",
      "Episode 31: Total Reward = 24.0\n",
      "Episode 32: Total Reward = 23.0\n",
      "Episode 33: Total Reward = 33.0\n",
      "Episode 34: Total Reward = 18.0\n",
      "Episode 35: Total Reward = 22.0\n",
      "Episode 36: Total Reward = 30.0\n",
      "Episode 37: Total Reward = 24.0\n",
      "Episode 38: Total Reward = 14.0\n",
      "Episode 39: Total Reward = 24.0\n",
      "Episode 40: Total Reward = 27.0\n",
      "Episode 41: Total Reward = 14.0\n",
      "Episode 42: Total Reward = 21.0\n",
      "Episode 43: Total Reward = 23.0\n",
      "Episode 44: Total Reward = 18.0\n",
      "Episode 45: Total Reward = 16.0\n",
      "Episode 46: Total Reward = 24.0\n",
      "Episode 47: Total Reward = 44.0\n",
      "Episode 48: Total Reward = 12.0\n",
      "Episode 49: Total Reward = 13.0\n",
      "Episode 50: Total Reward = 12.0\n",
      "Episode 51: Total Reward = 12.0\n",
      "Episode 52: Total Reward = 12.0\n",
      "Episode 53: Total Reward = 14.0\n",
      "Episode 54: Total Reward = 20.0\n",
      "Episode 55: Total Reward = 28.0\n",
      "Episode 56: Total Reward = 17.0\n",
      "Episode 57: Total Reward = 18.0\n",
      "Episode 58: Total Reward = 12.0\n",
      "Episode 59: Total Reward = 33.0\n",
      "Episode 60: Total Reward = 26.0\n",
      "Episode 61: Total Reward = 23.0\n",
      "Episode 62: Total Reward = 38.0\n",
      "Episode 63: Total Reward = 16.0\n",
      "Episode 64: Total Reward = 24.0\n",
      "Episode 65: Total Reward = 10.0\n",
      "Episode 66: Total Reward = 18.0\n",
      "Episode 67: Total Reward = 14.0\n",
      "Episode 68: Total Reward = 9.0\n",
      "Episode 69: Total Reward = 24.0\n",
      "Episode 70: Total Reward = 23.0\n",
      "Episode 71: Total Reward = 20.0\n",
      "Episode 72: Total Reward = 27.0\n",
      "Episode 73: Total Reward = 14.0\n",
      "Episode 74: Total Reward = 12.0\n",
      "Episode 75: Total Reward = 28.0\n",
      "Episode 76: Total Reward = 23.0\n",
      "Episode 77: Total Reward = 20.0\n",
      "Episode 78: Total Reward = 10.0\n",
      "Episode 79: Total Reward = 14.0\n",
      "Episode 80: Total Reward = 25.0\n",
      "Episode 81: Total Reward = 20.0\n",
      "Episode 82: Total Reward = 32.0\n",
      "Episode 83: Total Reward = 17.0\n",
      "Episode 84: Total Reward = 17.0\n",
      "Episode 85: Total Reward = 16.0\n",
      "Episode 86: Total Reward = 48.0\n",
      "Episode 87: Total Reward = 13.0\n",
      "Episode 88: Total Reward = 24.0\n",
      "Episode 89: Total Reward = 30.0\n",
      "Episode 90: Total Reward = 23.0\n",
      "Episode 91: Total Reward = 12.0\n",
      "Episode 92: Total Reward = 14.0\n",
      "Episode 93: Total Reward = 12.0\n",
      "Episode 94: Total Reward = 40.0\n",
      "Episode 95: Total Reward = 35.0\n",
      "Episode 96: Total Reward = 23.0\n",
      "Episode 97: Total Reward = 17.0\n",
      "Episode 98: Total Reward = 25.0\n",
      "Episode 99: Total Reward = 22.0\n",
      "Episode 100: Total Reward = 27.0\n",
      "Episode 101: Total Reward = 15.0\n",
      "Episode 102: Total Reward = 16.0\n",
      "Episode 103: Total Reward = 13.0\n",
      "Episode 104: Total Reward = 45.0\n",
      "Episode 105: Total Reward = 24.0\n",
      "Episode 106: Total Reward = 11.0\n",
      "Episode 107: Total Reward = 15.0\n",
      "Episode 108: Total Reward = 34.0\n",
      "Episode 109: Total Reward = 11.0\n",
      "Episode 110: Total Reward = 37.0\n",
      "Episode 111: Total Reward = 27.0\n",
      "Episode 112: Total Reward = 22.0\n",
      "Episode 113: Total Reward = 15.0\n",
      "Episode 114: Total Reward = 61.0\n",
      "Episode 115: Total Reward = 45.0\n",
      "Episode 116: Total Reward = 13.0\n",
      "Episode 117: Total Reward = 25.0\n",
      "Episode 118: Total Reward = 28.0\n",
      "Episode 119: Total Reward = 35.0\n",
      "Episode 120: Total Reward = 17.0\n",
      "Episode 121: Total Reward = 26.0\n",
      "Episode 122: Total Reward = 41.0\n",
      "Episode 123: Total Reward = 37.0\n",
      "Episode 124: Total Reward = 25.0\n",
      "Episode 125: Total Reward = 33.0\n",
      "Episode 126: Total Reward = 21.0\n",
      "Episode 127: Total Reward = 63.0\n",
      "Episode 128: Total Reward = 9.0\n",
      "Episode 129: Total Reward = 44.0\n",
      "Episode 130: Total Reward = 34.0\n",
      "Episode 131: Total Reward = 37.0\n",
      "Episode 132: Total Reward = 19.0\n",
      "Episode 133: Total Reward = 62.0\n",
      "Episode 134: Total Reward = 26.0\n",
      "Episode 135: Total Reward = 26.0\n",
      "Episode 136: Total Reward = 74.0\n",
      "Episode 137: Total Reward = 19.0\n",
      "Episode 138: Total Reward = 30.0\n",
      "Episode 139: Total Reward = 20.0\n",
      "Episode 140: Total Reward = 23.0\n",
      "Episode 141: Total Reward = 39.0\n",
      "Episode 142: Total Reward = 27.0\n",
      "Episode 143: Total Reward = 37.0\n",
      "Episode 144: Total Reward = 34.0\n",
      "Episode 145: Total Reward = 18.0\n",
      "Episode 146: Total Reward = 16.0\n",
      "Episode 147: Total Reward = 29.0\n",
      "Episode 148: Total Reward = 26.0\n",
      "Episode 149: Total Reward = 81.0\n",
      "Episode 150: Total Reward = 16.0\n",
      "Episode 151: Total Reward = 23.0\n",
      "Episode 152: Total Reward = 14.0\n",
      "Episode 153: Total Reward = 18.0\n",
      "Episode 154: Total Reward = 13.0\n",
      "Episode 155: Total Reward = 54.0\n",
      "Episode 156: Total Reward = 26.0\n",
      "Episode 157: Total Reward = 23.0\n",
      "Episode 158: Total Reward = 12.0\n",
      "Episode 159: Total Reward = 39.0\n",
      "Episode 160: Total Reward = 49.0\n",
      "Episode 161: Total Reward = 27.0\n",
      "Episode 162: Total Reward = 18.0\n",
      "Episode 163: Total Reward = 23.0\n",
      "Episode 164: Total Reward = 25.0\n",
      "Episode 165: Total Reward = 15.0\n",
      "Episode 166: Total Reward = 75.0\n",
      "Episode 167: Total Reward = 54.0\n",
      "Episode 168: Total Reward = 17.0\n",
      "Episode 169: Total Reward = 19.0\n",
      "Episode 170: Total Reward = 26.0\n",
      "Episode 171: Total Reward = 24.0\n",
      "Episode 172: Total Reward = 35.0\n",
      "Episode 173: Total Reward = 21.0\n",
      "Episode 174: Total Reward = 52.0\n",
      "Episode 175: Total Reward = 10.0\n",
      "Episode 176: Total Reward = 31.0\n",
      "Episode 177: Total Reward = 24.0\n",
      "Episode 178: Total Reward = 16.0\n",
      "Episode 179: Total Reward = 10.0\n",
      "Episode 180: Total Reward = 36.0\n",
      "Episode 181: Total Reward = 30.0\n",
      "Episode 182: Total Reward = 8.0\n",
      "Episode 183: Total Reward = 38.0\n",
      "Episode 184: Total Reward = 46.0\n",
      "Episode 185: Total Reward = 17.0\n",
      "Episode 186: Total Reward = 24.0\n",
      "Episode 187: Total Reward = 41.0\n",
      "Episode 188: Total Reward = 62.0\n",
      "Episode 189: Total Reward = 28.0\n",
      "Episode 190: Total Reward = 30.0\n",
      "Episode 191: Total Reward = 32.0\n",
      "Episode 192: Total Reward = 13.0\n",
      "Episode 193: Total Reward = 19.0\n",
      "Episode 194: Total Reward = 63.0\n",
      "Episode 195: Total Reward = 11.0\n",
      "Episode 196: Total Reward = 33.0\n",
      "Episode 197: Total Reward = 16.0\n",
      "Episode 198: Total Reward = 36.0\n",
      "Episode 199: Total Reward = 80.0\n",
      "Episode 200: Total Reward = 32.0\n",
      "Episode 201: Total Reward = 15.0\n",
      "Episode 202: Total Reward = 36.0\n",
      "Episode 203: Total Reward = 69.0\n",
      "Episode 204: Total Reward = 12.0\n",
      "Episode 205: Total Reward = 32.0\n",
      "Episode 206: Total Reward = 30.0\n",
      "Episode 207: Total Reward = 14.0\n",
      "Episode 208: Total Reward = 41.0\n",
      "Episode 209: Total Reward = 19.0\n",
      "Episode 210: Total Reward = 51.0\n",
      "Episode 211: Total Reward = 13.0\n",
      "Episode 212: Total Reward = 17.0\n",
      "Episode 213: Total Reward = 16.0\n",
      "Episode 214: Total Reward = 15.0\n",
      "Episode 215: Total Reward = 24.0\n",
      "Episode 216: Total Reward = 16.0\n",
      "Episode 217: Total Reward = 13.0\n",
      "Episode 218: Total Reward = 65.0\n",
      "Episode 219: Total Reward = 16.0\n",
      "Episode 220: Total Reward = 19.0\n",
      "Episode 221: Total Reward = 19.0\n",
      "Episode 222: Total Reward = 39.0\n",
      "Episode 223: Total Reward = 26.0\n",
      "Episode 224: Total Reward = 27.0\n",
      "Episode 225: Total Reward = 22.0\n",
      "Episode 226: Total Reward = 29.0\n",
      "Episode 227: Total Reward = 17.0\n",
      "Episode 228: Total Reward = 26.0\n",
      "Episode 229: Total Reward = 19.0\n",
      "Episode 230: Total Reward = 34.0\n",
      "Episode 231: Total Reward = 11.0\n",
      "Episode 232: Total Reward = 32.0\n",
      "Episode 233: Total Reward = 44.0\n",
      "Episode 234: Total Reward = 24.0\n",
      "Episode 235: Total Reward = 82.0\n",
      "Episode 236: Total Reward = 28.0\n",
      "Episode 237: Total Reward = 19.0\n",
      "Episode 238: Total Reward = 48.0\n",
      "Episode 239: Total Reward = 133.0\n",
      "Episode 240: Total Reward = 176.0\n",
      "Episode 241: Total Reward = 10.0\n",
      "Episode 242: Total Reward = 55.0\n",
      "Episode 243: Total Reward = 16.0\n",
      "Episode 244: Total Reward = 71.0\n",
      "Episode 245: Total Reward = 40.0\n",
      "Episode 246: Total Reward = 44.0\n",
      "Episode 247: Total Reward = 21.0\n",
      "Episode 248: Total Reward = 47.0\n",
      "Episode 249: Total Reward = 42.0\n",
      "Episode 250: Total Reward = 17.0\n",
      "Episode 251: Total Reward = 67.0\n",
      "Episode 252: Total Reward = 126.0\n",
      "Episode 253: Total Reward = 56.0\n",
      "Episode 254: Total Reward = 21.0\n",
      "Episode 255: Total Reward = 13.0\n",
      "Episode 256: Total Reward = 38.0\n",
      "Episode 257: Total Reward = 28.0\n",
      "Episode 258: Total Reward = 30.0\n",
      "Episode 259: Total Reward = 21.0\n",
      "Episode 260: Total Reward = 49.0\n",
      "Episode 261: Total Reward = 46.0\n",
      "Episode 262: Total Reward = 47.0\n",
      "Episode 263: Total Reward = 57.0\n",
      "Episode 264: Total Reward = 86.0\n",
      "Episode 265: Total Reward = 48.0\n",
      "Episode 266: Total Reward = 88.0\n",
      "Episode 267: Total Reward = 13.0\n",
      "Episode 268: Total Reward = 24.0\n",
      "Episode 269: Total Reward = 91.0\n",
      "Episode 270: Total Reward = 24.0\n",
      "Episode 271: Total Reward = 13.0\n",
      "Episode 272: Total Reward = 90.0\n",
      "Episode 273: Total Reward = 49.0\n",
      "Episode 274: Total Reward = 33.0\n",
      "Episode 275: Total Reward = 12.0\n",
      "Episode 276: Total Reward = 13.0\n",
      "Episode 277: Total Reward = 25.0\n",
      "Episode 278: Total Reward = 69.0\n",
      "Episode 279: Total Reward = 43.0\n",
      "Episode 280: Total Reward = 23.0\n",
      "Episode 281: Total Reward = 41.0\n",
      "Episode 282: Total Reward = 13.0\n",
      "Episode 283: Total Reward = 36.0\n",
      "Episode 284: Total Reward = 47.0\n",
      "Episode 285: Total Reward = 90.0\n",
      "Episode 286: Total Reward = 93.0\n",
      "Episode 287: Total Reward = 23.0\n",
      "Episode 288: Total Reward = 52.0\n",
      "Episode 289: Total Reward = 27.0\n",
      "Episode 290: Total Reward = 46.0\n",
      "Episode 291: Total Reward = 27.0\n",
      "Episode 292: Total Reward = 58.0\n",
      "Episode 293: Total Reward = 43.0\n",
      "Episode 294: Total Reward = 31.0\n",
      "Episode 295: Total Reward = 59.0\n",
      "Episode 296: Total Reward = 11.0\n",
      "Episode 297: Total Reward = 43.0\n",
      "Episode 298: Total Reward = 35.0\n",
      "Episode 299: Total Reward = 92.0\n",
      "Episode 300: Total Reward = 26.0\n",
      "Episode 301: Total Reward = 14.0\n",
      "Episode 302: Total Reward = 80.0\n",
      "Episode 303: Total Reward = 66.0\n",
      "Episode 304: Total Reward = 20.0\n",
      "Episode 305: Total Reward = 20.0\n",
      "Episode 306: Total Reward = 122.0\n",
      "Episode 307: Total Reward = 13.0\n",
      "Episode 308: Total Reward = 59.0\n",
      "Episode 309: Total Reward = 29.0\n",
      "Episode 310: Total Reward = 98.0\n",
      "Episode 311: Total Reward = 22.0\n",
      "Episode 312: Total Reward = 16.0\n",
      "Episode 313: Total Reward = 20.0\n",
      "Episode 314: Total Reward = 65.0\n",
      "Episode 315: Total Reward = 23.0\n",
      "Episode 316: Total Reward = 33.0\n",
      "Episode 317: Total Reward = 28.0\n",
      "Episode 318: Total Reward = 38.0\n",
      "Episode 319: Total Reward = 88.0\n",
      "Episode 320: Total Reward = 30.0\n",
      "Episode 321: Total Reward = 32.0\n",
      "Episode 322: Total Reward = 78.0\n",
      "Episode 323: Total Reward = 14.0\n",
      "Episode 324: Total Reward = 55.0\n",
      "Episode 325: Total Reward = 14.0\n",
      "Episode 326: Total Reward = 27.0\n",
      "Episode 327: Total Reward = 23.0\n",
      "Episode 328: Total Reward = 33.0\n",
      "Episode 329: Total Reward = 18.0\n",
      "Episode 330: Total Reward = 64.0\n",
      "Episode 331: Total Reward = 40.0\n",
      "Episode 332: Total Reward = 94.0\n",
      "Episode 333: Total Reward = 20.0\n",
      "Episode 334: Total Reward = 11.0\n",
      "Episode 335: Total Reward = 50.0\n",
      "Episode 336: Total Reward = 65.0\n",
      "Episode 337: Total Reward = 26.0\n",
      "Episode 338: Total Reward = 22.0\n",
      "Episode 339: Total Reward = 35.0\n",
      "Episode 340: Total Reward = 28.0\n",
      "Episode 341: Total Reward = 112.0\n",
      "Episode 342: Total Reward = 24.0\n",
      "Episode 343: Total Reward = 70.0\n",
      "Episode 344: Total Reward = 42.0\n",
      "Episode 345: Total Reward = 49.0\n",
      "Episode 346: Total Reward = 136.0\n",
      "Episode 347: Total Reward = 27.0\n",
      "Episode 348: Total Reward = 126.0\n",
      "Episode 349: Total Reward = 37.0\n",
      "Episode 350: Total Reward = 14.0\n",
      "Episode 351: Total Reward = 129.0\n",
      "Episode 352: Total Reward = 35.0\n",
      "Episode 353: Total Reward = 75.0\n",
      "Episode 354: Total Reward = 29.0\n",
      "Episode 355: Total Reward = 17.0\n",
      "Episode 356: Total Reward = 38.0\n",
      "Episode 357: Total Reward = 173.0\n",
      "Episode 358: Total Reward = 41.0\n",
      "Episode 359: Total Reward = 23.0\n",
      "Episode 360: Total Reward = 19.0\n",
      "Episode 361: Total Reward = 25.0\n",
      "Episode 362: Total Reward = 19.0\n",
      "Episode 363: Total Reward = 28.0\n",
      "Episode 364: Total Reward = 113.0\n",
      "Episode 365: Total Reward = 39.0\n",
      "Episode 366: Total Reward = 65.0\n",
      "Episode 367: Total Reward = 20.0\n",
      "Episode 368: Total Reward = 21.0\n",
      "Episode 369: Total Reward = 114.0\n",
      "Episode 370: Total Reward = 95.0\n",
      "Episode 371: Total Reward = 44.0\n",
      "Episode 372: Total Reward = 25.0\n",
      "Episode 373: Total Reward = 25.0\n",
      "Episode 374: Total Reward = 17.0\n",
      "Episode 375: Total Reward = 86.0\n",
      "Episode 376: Total Reward = 31.0\n",
      "Episode 377: Total Reward = 35.0\n",
      "Episode 378: Total Reward = 37.0\n",
      "Episode 379: Total Reward = 13.0\n",
      "Episode 380: Total Reward = 104.0\n",
      "Episode 381: Total Reward = 69.0\n",
      "Episode 382: Total Reward = 32.0\n",
      "Episode 383: Total Reward = 26.0\n",
      "Episode 384: Total Reward = 66.0\n",
      "Episode 385: Total Reward = 39.0\n",
      "Episode 386: Total Reward = 59.0\n",
      "Episode 387: Total Reward = 40.0\n",
      "Episode 388: Total Reward = 23.0\n",
      "Episode 389: Total Reward = 114.0\n",
      "Episode 390: Total Reward = 27.0\n",
      "Episode 391: Total Reward = 18.0\n",
      "Episode 392: Total Reward = 133.0\n",
      "Episode 393: Total Reward = 60.0\n",
      "Episode 394: Total Reward = 19.0\n",
      "Episode 395: Total Reward = 11.0\n",
      "Episode 396: Total Reward = 150.0\n",
      "Episode 397: Total Reward = 20.0\n",
      "Episode 398: Total Reward = 34.0\n",
      "Episode 399: Total Reward = 21.0\n",
      "Episode 400: Total Reward = 25.0\n",
      "Episode 401: Total Reward = 20.0\n",
      "Episode 402: Total Reward = 81.0\n",
      "Episode 403: Total Reward = 33.0\n",
      "Episode 404: Total Reward = 17.0\n",
      "Episode 405: Total Reward = 32.0\n",
      "Episode 406: Total Reward = 94.0\n",
      "Episode 407: Total Reward = 14.0\n",
      "Episode 408: Total Reward = 93.0\n",
      "Episode 409: Total Reward = 148.0\n",
      "Episode 410: Total Reward = 18.0\n",
      "Episode 411: Total Reward = 50.0\n",
      "Episode 412: Total Reward = 49.0\n",
      "Episode 413: Total Reward = 105.0\n",
      "Episode 414: Total Reward = 62.0\n",
      "Episode 415: Total Reward = 15.0\n",
      "Episode 416: Total Reward = 30.0\n",
      "Episode 417: Total Reward = 54.0\n",
      "Episode 418: Total Reward = 37.0\n",
      "Episode 419: Total Reward = 75.0\n",
      "Episode 420: Total Reward = 54.0\n",
      "Episode 421: Total Reward = 75.0\n",
      "Episode 422: Total Reward = 108.0\n",
      "Episode 423: Total Reward = 11.0\n",
      "Episode 424: Total Reward = 140.0\n",
      "Episode 425: Total Reward = 72.0\n",
      "Episode 426: Total Reward = 32.0\n",
      "Episode 427: Total Reward = 44.0\n",
      "Episode 428: Total Reward = 44.0\n",
      "Episode 429: Total Reward = 134.0\n",
      "Episode 430: Total Reward = 91.0\n",
      "Episode 431: Total Reward = 104.0\n",
      "Episode 432: Total Reward = 76.0\n",
      "Episode 433: Total Reward = 32.0\n",
      "Episode 434: Total Reward = 24.0\n",
      "Episode 435: Total Reward = 170.0\n",
      "Episode 436: Total Reward = 60.0\n",
      "Episode 437: Total Reward = 49.0\n",
      "Episode 438: Total Reward = 226.0\n",
      "Episode 439: Total Reward = 10.0\n",
      "Episode 440: Total Reward = 74.0\n",
      "Episode 441: Total Reward = 42.0\n",
      "Episode 442: Total Reward = 177.0\n",
      "Episode 443: Total Reward = 155.0\n",
      "Episode 444: Total Reward = 22.0\n",
      "Episode 445: Total Reward = 38.0\n",
      "Episode 446: Total Reward = 89.0\n",
      "Episode 447: Total Reward = 82.0\n",
      "Episode 448: Total Reward = 28.0\n",
      "Episode 449: Total Reward = 93.0\n",
      "Episode 450: Total Reward = 17.0\n",
      "Episode 451: Total Reward = 80.0\n",
      "Episode 452: Total Reward = 45.0\n",
      "Episode 453: Total Reward = 51.0\n",
      "Episode 454: Total Reward = 100.0\n",
      "Episode 455: Total Reward = 65.0\n",
      "Episode 456: Total Reward = 17.0\n",
      "Episode 457: Total Reward = 86.0\n",
      "Episode 458: Total Reward = 75.0\n",
      "Episode 459: Total Reward = 18.0\n",
      "Episode 460: Total Reward = 51.0\n",
      "Episode 461: Total Reward = 159.0\n",
      "Episode 462: Total Reward = 85.0\n",
      "Episode 463: Total Reward = 129.0\n",
      "Episode 464: Total Reward = 68.0\n",
      "Episode 465: Total Reward = 29.0\n",
      "Episode 466: Total Reward = 125.0\n",
      "Episode 467: Total Reward = 18.0\n",
      "Episode 468: Total Reward = 61.0\n",
      "Episode 469: Total Reward = 78.0\n",
      "Episode 470: Total Reward = 16.0\n",
      "Episode 471: Total Reward = 51.0\n",
      "Episode 472: Total Reward = 68.0\n",
      "Episode 473: Total Reward = 81.0\n",
      "Episode 474: Total Reward = 121.0\n",
      "Episode 475: Total Reward = 15.0\n",
      "Episode 476: Total Reward = 91.0\n",
      "Episode 477: Total Reward = 22.0\n",
      "Episode 478: Total Reward = 239.0\n",
      "Episode 479: Total Reward = 54.0\n",
      "Episode 480: Total Reward = 131.0\n",
      "Episode 481: Total Reward = 37.0\n",
      "Episode 482: Total Reward = 17.0\n",
      "Episode 483: Total Reward = 141.0\n",
      "Episode 484: Total Reward = 36.0\n",
      "Episode 485: Total Reward = 51.0\n",
      "Episode 486: Total Reward = 39.0\n",
      "Episode 487: Total Reward = 134.0\n",
      "Episode 488: Total Reward = 14.0\n",
      "Episode 489: Total Reward = 82.0\n",
      "Episode 490: Total Reward = 109.0\n",
      "Episode 491: Total Reward = 57.0\n",
      "Episode 492: Total Reward = 153.0\n",
      "Episode 493: Total Reward = 25.0\n",
      "Episode 494: Total Reward = 195.0\n",
      "Episode 495: Total Reward = 140.0\n",
      "Episode 496: Total Reward = 66.0\n",
      "Episode 497: Total Reward = 57.0\n",
      "Episode 498: Total Reward = 103.0\n",
      "Episode 499: Total Reward = 57.0\n",
      "Episode 500: Total Reward = 151.0\n"
     ]
    }
   ],
   "source": [
    "max_reward = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = choose_action(state, epsilon)  # Choose action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Store the experience\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Train the model\n",
    "        train_model()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Sync weights with the target model periodically\n",
    "    if episode % 20 == 0:\n",
    "        target_model.set_weights(model.get_weights())\n",
    "\n",
    "    if total_reward > max_reward:\n",
    "        model.save(f\"model/dqn-model-max.keras\")\n",
    "        max_reward = total_reward\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/dqn-model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trained!\n",
    "\n",
    "Congratulations! You have trained a reinforcement model. To visualize its performance, run the `test.py` file. You can also replace `model/dqn-model.keras` with `model/dqn-model-max.keras` to see how the two models compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
